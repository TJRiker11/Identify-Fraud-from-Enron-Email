{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Fraud from Enron Email"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "> In 2000, Enron was one of the largest companies in the United States. By 2002, it had collapsed into bankruptcy due to widespread corporate fraud. In the resulting Federal investigation, a significant amount of typically confidential information entered into the public record, including tens of thousands of emails and detailed financial data for top executives. This project is to build a person of interest identifier based on financial and email data made public as a result of the Enron scandal.\n",
    "\n",
    "> A Person of Interest(PoI) is an individual who meets one of the following criteria:\n",
    ">* Individuals who were indicted\n",
    ">* Individuals who reached a settlement or plea deal with the government\n",
    ">* Individuals who testified in exchange for prosecution immunity.\n",
    "\n",
    "> The full project and all data for this project can be seen in [this Github repository](https://github.com/TrikerDev/Identify-Fraud-from-Enron-Email). Data such as financial records, emails from employees, PoI names, etc, can be seen [here](https://github.com/TrikerDev/Identify-Fraud-from-Enron-Email/tree/master/Identify%20Fraud%20from%20Enron%20Email/Enron%20Data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing packages\n",
    "import sys\n",
    "import pickle\n",
    "sys.path.append(\"Enron Data/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "with open(\"Enron Data/final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The number of executives in the dataset\n",
    "len(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['METTS MARK', 'BAXTER JOHN C', 'ELLIOTT STEVEN', 'CORDES WILLIAM R', 'HANNON KEVIN P', 'MORDAUNT KRISTINA M', 'MEYER ROCKFORD G', 'MCMAHON JEFFREY', 'HORTON STANLEY C', 'PIPER GREGORY F', 'HUMPHREY GENE E', 'UMANOFF ADAM S', 'BLACHMAN JEREMY M', 'SUNDE MARTIN', 'GIBBS DANA R', 'LOWRY CHARLES P', 'COLWELL WESLEY', 'MULLER MARK S', 'JACKSON CHARLENE R', 'WESTFAHL RICHARD K', 'WALTERS GARETH W', 'WALLS JR ROBERT H', 'KITCHEN LOUISE', 'CHAN RONNIE', 'BELFER ROBERT', 'SHANKMAN JEFFREY A', 'WODRASKA JOHN', 'BERGSIEKER RICHARD P', 'URQUHART JOHN A', 'BIBI PHILIPPE A', 'RIEKER PAULA H', 'WHALEY DAVID A', 'BECK SALLY W', 'HAUG DAVID L', 'ECHOLS JOHN B', 'MENDELSOHN JOHN', 'HICKERSON GARY J', 'CLINE KENNETH W', 'LEWIS RICHARD', 'HAYES ROBERT E', 'MCCARTY DANNY J', 'KOPPER MICHAEL J', 'LEFF DANIEL P', 'LAVORATO JOHN J', 'BERBERIAN DAVID', 'DETMERING TIMOTHY J', 'WAKEHAM JOHN', 'POWERS WILLIAM', 'GOLD JOSEPH', 'BANNANTINE JAMES M', 'DUNCAN JOHN H', 'SHAPIRO RICHARD S', 'SHERRIFF JOHN R', 'SHELBY REX', 'LEMAISTRE CHARLES', 'DEFFNER JOSEPH M', 'KISHKILL JOSEPH G', 'WHALLEY LAWRENCE G', 'MCCONNELL MICHAEL S', 'PIRO JIM', 'DELAINEY DAVID W', 'SULLIVAN-SHAKLOVITZ COLLEEN', 'WROBEL BRUCE', 'LINDHOLM TOD A', 'MEYER JEROME J', 'LAY KENNETH L', 'BUTTS ROBERT H', 'OLSON CINDY K', 'MCDONALD REBECCA', 'CUMBERLAND MICHAEL S', 'GAHN ROBERT S', 'MCCLELLAN GEORGE', 'HERMANN ROBERT J', 'SCRIMSHAW MATTHEW', 'GATHMANN WILLIAM D', 'HAEDICKE MARK E', 'BOWEN JR RAYMOND M', 'GILLIS JOHN', 'FITZGERALD JAY L', 'MORAN MICHAEL P', 'REDMOND BRIAN L', 'BAZELIDES PHILIP J', 'BELDEN TIMOTHY N', 'DURAN WILLIAM D', 'THORN TERENCE H', 'FASTOW ANDREW S', 'FOY JOE', 'CALGER CHRISTOPHER F', 'RICE KENNETH D', 'KAMINSKI WINCENTY J', 'LOCKHART EUGENE E', 'COX DAVID', 'OVERDYKE JR JERE C', 'PEREIRA PAULO V. FERRAZ', 'STABLER FRANK', 'SKILLING JEFFREY K', 'BLAKE JR. NORMAN P', 'SHERRICK JEFFREY B', 'PRENTICE JAMES', 'GRAY RODNEY', 'PICKERING MARK R', 'THE TRAVEL AGENCY IN THE PARK', 'NOLES JAMES L', 'KEAN STEVEN J', 'TOTAL', 'FOWLER PEGGY', 'WASAFF GEORGE', 'WHITE JR THOMAS E', 'CHRISTODOULOU DIOMEDES', 'ALLEN PHILLIP K', 'SHARP VICTORIA T', 'JAEDICKE ROBERT', 'WINOKUR JR. HERBERT S', 'BROWN MICHAEL', 'BADUM JAMES P', 'HUGHES JAMES A', 'REYNOLDS LAWRENCE', 'DIMICHELE RICHARD G', 'BHATNAGAR SANJAY', 'CARTER REBECCA C', 'BUCHANAN HAROLD G', 'YEAP SOON', 'MURRAY JULIA H', 'GARLAND C KEVIN', 'DODSON KEITH', 'YEAGER F SCOTT', 'HIRKO JOSEPH', 'DIETRICH JANET R', 'DERRICK JR. JAMES V', 'FREVERT MARK A', 'PAI LOU L', 'BAY FRANKLIN R', 'HAYSLETT RODERICK J', 'FUGH JOHN L', 'FALLON JAMES B', 'KOENIG MARK E', 'SAVAGE FRANK', 'IZZO LAWRENCE L', 'TILNEY ELIZABETH A', 'MARTIN AMANDA K', 'BUY RICHARD B', 'GRAMM WENDY L', 'CAUSEY RICHARD A', 'TAYLOR MITCHELL S', 'DONAHUE JR JEFFREY M', 'GLISAN JR BEN F']\n"
     ]
    }
   ],
   "source": [
    "# The names of the executives\n",
    "print data_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Right away, we can see some outliers in this dataset. The first is 'THE TRAVEL AGENCY IN THE PARK'. This is supposed to be a list of the names of executives, so this is clearly in the wrong place. Another outlier is 'TOTAL'. This 'TOTAL' data is the sum of all other executives. We dont want the total of every person on the list, so this is an outlier that will be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bonus': 'NaN',\n",
       " 'deferral_payments': 'NaN',\n",
       " 'deferred_income': 'NaN',\n",
       " 'director_fees': 'NaN',\n",
       " 'email_address': 'NaN',\n",
       " 'exercised_stock_options': 'NaN',\n",
       " 'expenses': 'NaN',\n",
       " 'from_messages': 'NaN',\n",
       " 'from_poi_to_this_person': 'NaN',\n",
       " 'from_this_person_to_poi': 'NaN',\n",
       " 'loan_advances': 'NaN',\n",
       " 'long_term_incentive': 'NaN',\n",
       " 'other': 362096,\n",
       " 'poi': False,\n",
       " 'restricted_stock': 'NaN',\n",
       " 'restricted_stock_deferred': 'NaN',\n",
       " 'salary': 'NaN',\n",
       " 'shared_receipt_with_poi': 'NaN',\n",
       " 'to_messages': 'NaN',\n",
       " 'total_payments': 362096,\n",
       " 'total_stock_value': 'NaN'}"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing THE TRAVEL AGENCY IN THE PARK outlier\n",
    "data_dict.pop('THE TRAVEL AGENCY IN THE PARK', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bonus': 97343619,\n",
       " 'deferral_payments': 32083396,\n",
       " 'deferred_income': -27992891,\n",
       " 'director_fees': 1398517,\n",
       " 'email_address': 'NaN',\n",
       " 'exercised_stock_options': 311764000,\n",
       " 'expenses': 5235198,\n",
       " 'from_messages': 'NaN',\n",
       " 'from_poi_to_this_person': 'NaN',\n",
       " 'from_this_person_to_poi': 'NaN',\n",
       " 'loan_advances': 83925000,\n",
       " 'long_term_incentive': 48521928,\n",
       " 'other': 42667589,\n",
       " 'poi': False,\n",
       " 'restricted_stock': 130322299,\n",
       " 'restricted_stock_deferred': -7576788,\n",
       " 'salary': 26704229,\n",
       " 'shared_receipt_with_poi': 'NaN',\n",
       " 'to_messages': 'NaN',\n",
       " 'total_payments': 309886585,\n",
       " 'total_stock_value': 434509511}"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing TOTAL outlier\n",
    "data_dict.pop('TOTAL', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Another way to find potential outliers is to find individuals who may have Total Payments as NaN. This could potentially mean that this person should not be included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORDES WILLIAM R\n",
      "LOWRY CHARLES P\n",
      "CHAN RONNIE\n",
      "WHALEY DAVID A\n",
      "CLINE KENNETH W\n",
      "LEWIS RICHARD\n",
      "MCCARTY DANNY J\n",
      "POWERS WILLIAM\n",
      "PIRO JIM\n",
      "WROBEL BRUCE\n",
      "MCDONALD REBECCA\n",
      "SCRIMSHAW MATTHEW\n",
      "GATHMANN WILLIAM D\n",
      "GILLIS JOHN\n",
      "MORAN MICHAEL P\n",
      "LOCKHART EUGENE E\n",
      "SHERRICK JEFFREY B\n",
      "FOWLER PEGGY\n",
      "CHRISTODOULOU DIOMEDES\n",
      "HUGHES JAMES A\n",
      "HAYSLETT RODERICK J\n"
     ]
    }
   ],
   "source": [
    "# List of people with no total payment data\n",
    "for entry in data_dict:\n",
    "    if data_dict[entry]['total_payments'] == 'NaN':\n",
    "        print entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> This could be narrowed down further by adding another main criteria to the list: the stock options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHAN RONNIE\n",
      "POWERS WILLIAM\n",
      "LOCKHART EUGENE E\n"
     ]
    }
   ],
   "source": [
    "# List of people with no total payment data and no stock option data\n",
    "for entry in data_dict:\n",
    "    if data_dict[entry]['total_payments'] == 'NaN' and data_dict[entry]['total_stock_value'] == 'NaN':\n",
    "        print entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Investigating these individuals, we can see that they are missing lots of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bonus': 'NaN',\n",
       " 'deferral_payments': 'NaN',\n",
       " 'deferred_income': -98784,\n",
       " 'director_fees': 98784,\n",
       " 'email_address': 'NaN',\n",
       " 'exercised_stock_options': 'NaN',\n",
       " 'expenses': 'NaN',\n",
       " 'from_messages': 'NaN',\n",
       " 'from_poi_to_this_person': 'NaN',\n",
       " 'from_this_person_to_poi': 'NaN',\n",
       " 'loan_advances': 'NaN',\n",
       " 'long_term_incentive': 'NaN',\n",
       " 'other': 'NaN',\n",
       " 'poi': False,\n",
       " 'restricted_stock': 32460,\n",
       " 'restricted_stock_deferred': -32460,\n",
       " 'salary': 'NaN',\n",
       " 'shared_receipt_with_poi': 'NaN',\n",
       " 'to_messages': 'NaN',\n",
       " 'total_payments': 'NaN',\n",
       " 'total_stock_value': 'NaN'}"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Investigating CHAN RONNIE\n",
    "data_dict['CHAN RONNIE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bonus': 'NaN',\n",
       " 'deferral_payments': 'NaN',\n",
       " 'deferred_income': -17500,\n",
       " 'director_fees': 17500,\n",
       " 'email_address': 'ken.powers@enron.com',\n",
       " 'exercised_stock_options': 'NaN',\n",
       " 'expenses': 'NaN',\n",
       " 'from_messages': 26,\n",
       " 'from_poi_to_this_person': 0,\n",
       " 'from_this_person_to_poi': 0,\n",
       " 'loan_advances': 'NaN',\n",
       " 'long_term_incentive': 'NaN',\n",
       " 'other': 'NaN',\n",
       " 'poi': False,\n",
       " 'restricted_stock': 'NaN',\n",
       " 'restricted_stock_deferred': 'NaN',\n",
       " 'salary': 'NaN',\n",
       " 'shared_receipt_with_poi': 12,\n",
       " 'to_messages': 653,\n",
       " 'total_payments': 'NaN',\n",
       " 'total_stock_value': 'NaN'}"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Investigating POWERS WILLIAM\n",
    "data_dict['POWERS WILLIAM']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bonus': 'NaN',\n",
       " 'deferral_payments': 'NaN',\n",
       " 'deferred_income': 'NaN',\n",
       " 'director_fees': 'NaN',\n",
       " 'email_address': 'NaN',\n",
       " 'exercised_stock_options': 'NaN',\n",
       " 'expenses': 'NaN',\n",
       " 'from_messages': 'NaN',\n",
       " 'from_poi_to_this_person': 'NaN',\n",
       " 'from_this_person_to_poi': 'NaN',\n",
       " 'loan_advances': 'NaN',\n",
       " 'long_term_incentive': 'NaN',\n",
       " 'other': 'NaN',\n",
       " 'poi': False,\n",
       " 'restricted_stock': 'NaN',\n",
       " 'restricted_stock_deferred': 'NaN',\n",
       " 'salary': 'NaN',\n",
       " 'shared_receipt_with_poi': 'NaN',\n",
       " 'to_messages': 'NaN',\n",
       " 'total_payments': 'NaN',\n",
       " 'total_stock_value': 'NaN'}"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Investigating LOCKHART EUGENE E\n",
    "data_dict['LOCKHART EUGENE E']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LOCKHART EUGENE E is the most obvious outlier here, as this individual has NaN for every feature, and he is also not a PoI. This is an outlier that will be removed.\n",
    "\n",
    "> CHAN RONNIE is also an outlier, as he as NaN for most features and is not a PoI. The features he does have are payments and stock. However, these are both cancelled out to zero as they are deferred for the exact same amount. This leaves us with another blank slate. This outlier will be removed.\n",
    "\n",
    "> POWERS WILLIAM does have NaN and 0 for many of the features, however he does have several features that the other two did not have, such as messages and receipts. This gives up more information that can be used in the investigation. So even though much information is missing, we can still gather some data from this individual. This is not an outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bonus': 'NaN',\n",
       " 'deferral_payments': 'NaN',\n",
       " 'deferred_income': 'NaN',\n",
       " 'director_fees': 'NaN',\n",
       " 'email_address': 'NaN',\n",
       " 'exercised_stock_options': 'NaN',\n",
       " 'expenses': 'NaN',\n",
       " 'from_messages': 'NaN',\n",
       " 'from_poi_to_this_person': 'NaN',\n",
       " 'from_this_person_to_poi': 'NaN',\n",
       " 'loan_advances': 'NaN',\n",
       " 'long_term_incentive': 'NaN',\n",
       " 'other': 'NaN',\n",
       " 'poi': False,\n",
       " 'restricted_stock': 'NaN',\n",
       " 'restricted_stock_deferred': 'NaN',\n",
       " 'salary': 'NaN',\n",
       " 'shared_receipt_with_poi': 'NaN',\n",
       " 'to_messages': 'NaN',\n",
       " 'total_payments': 'NaN',\n",
       " 'total_stock_value': 'NaN'}"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing LOCKHART EUGENE E outlier\n",
    "data_dict.pop('LOCKHART EUGENE E', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bonus': 'NaN',\n",
       " 'deferral_payments': 'NaN',\n",
       " 'deferred_income': -98784,\n",
       " 'director_fees': 98784,\n",
       " 'email_address': 'NaN',\n",
       " 'exercised_stock_options': 'NaN',\n",
       " 'expenses': 'NaN',\n",
       " 'from_messages': 'NaN',\n",
       " 'from_poi_to_this_person': 'NaN',\n",
       " 'from_this_person_to_poi': 'NaN',\n",
       " 'loan_advances': 'NaN',\n",
       " 'long_term_incentive': 'NaN',\n",
       " 'other': 'NaN',\n",
       " 'poi': False,\n",
       " 'restricted_stock': 32460,\n",
       " 'restricted_stock_deferred': -32460,\n",
       " 'salary': 'NaN',\n",
       " 'shared_receipt_with_poi': 'NaN',\n",
       " 'to_messages': 'NaN',\n",
       " 'total_payments': 'NaN',\n",
       " 'total_stock_value': 'NaN'}"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Removing CHAN RONNIE outlier\n",
    "data_dict.pop('CHAN RONNIE', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving this new dataset without the outliers\n",
    "my_dataset = data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In this section we are going to create a new feature to add to the feature list. The original features will be put into a list below, and then a new feature will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "['salary', 'to_messages', 'deferral_payments', 'total_payments', 'exercised_stock_options', 'bonus', 'restricted_stock', 'shared_receipt_with_poi', 'restricted_stock_deferred', 'total_stock_value', 'expenses', 'loan_advances', 'from_messages', 'other', 'from_this_person_to_poi', 'poi', 'director_fees', 'deferred_income', 'long_term_incentive', 'email_address', 'from_poi_to_this_person']\n"
     ]
    }
   ],
   "source": [
    "# Printing all features\n",
    "print len((my_dataset['SKILLING JEFFREY K'].keys()))\n",
    "print (my_dataset['SKILLING JEFFREY K'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can see that there are 21 original features. We are going to add these into a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of original features\n",
    "features_list = [\n",
    " 'poi',\n",
    " 'bonus',\n",
    " 'deferral_payments',\n",
    " 'deferred_income',\n",
    " 'director_fees',\n",
    " 'email_address',\n",
    " 'exercised_stock_options',\n",
    " 'expenses',\n",
    " 'from_messages',\n",
    " 'from_poi_to_this_person',\n",
    " 'from_this_person_to_poi',\n",
    " 'loan_advances',\n",
    " 'long_term_incentive',\n",
    " 'other',\n",
    " 'restricted_stock',\n",
    " 'restricted_stock_deferred',\n",
    " 'salary',\n",
    " 'shared_receipt_with_poi',\n",
    " 'to_messages',\n",
    " 'total_payments',\n",
    " 'total_stock_value'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The new feature we will create will be a cross between 'from_poi_to_this_person' and 'from_this_person_to_poi'. The theory is that a PoI would potentially have many more emails to and from other PoIs, whereas non PoIs would probably communicate less to PoIs. \n",
    "\n",
    "> We will create this new feature by adding the 'from_poi_to_this_person' and 'from_this_person_to_poi' together to get the total amount of PoI emails. We will then add 'to_messages' and 'from_messages' together to get the total amount of emails in general. Then dividing PoI emails by Total emails, we can get the percentage of communication between PoIs and Non-PoIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a new list by adding two lists together\n",
    "def get_total_list(list1, list2):\n",
    "    new_list = []\n",
    "    for i in my_dataset:\n",
    "        if my_dataset[i][list1] == 'NaN' or my_dataset[i][list2] == 'NaN':\n",
    "            new_list.append(0.)\n",
    "        elif my_dataset[i][list1]>=0:\n",
    "            new_list.append(float(my_dataset[i][list1]) + float(my_dataset[i][list2]))\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total PoI emails list\n",
    "poi_emails_list = get_total_list('from_this_person_to_poi', 'from_poi_to_this_person')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total emails list\n",
    "total_emails_list = get_total_list('to_messages', 'from_messages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divides one list by another list\n",
    "def fraction_list(list1, list2):\n",
    "    new_list = []\n",
    "    for i in range(0,len(list1)):\n",
    "        if list2[i] == 0.0:\n",
    "            new_list.append(0.0)\n",
    "        else:\n",
    "            new_list.append(float(list1[i])/float(list2[i]))\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting new list by dividing previously created lists\n",
    "fraction_poi_emails = fraction_list(poi_emails_list, total_emails_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding this new feature to the dataset\n",
    "count = 0\n",
    "for i in my_dataset:\n",
    "    my_dataset[i]['fraction_poi_emails'] = fraction_poi_emails[count]\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "['to_messages', 'deferral_payments', 'expenses', 'poi', 'deferred_income', 'email_address', 'long_term_incentive', 'restricted_stock_deferred', 'from_messages', 'shared_receipt_with_poi', 'loan_advances', 'fraction_poi_emails', 'other', 'director_fees', 'bonus', 'total_stock_value', 'from_poi_to_this_person', 'from_this_person_to_poi', 'restricted_stock', 'salary', 'total_payments', 'exercised_stock_options']\n"
     ]
    }
   ],
   "source": [
    "# printing all features\n",
    "print len((my_dataset['SKILLING JEFFREY K'].keys()))\n",
    "print (my_dataset['SKILLING JEFFREY K'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Printing all features again, we can now see that there are 22 features, as 'fraction_poi_emails' has been added to the list. Now we are going to add this new feature into the features_list.\n",
    "\n",
    "> However, the 'email_address' feature is actually unnecessary so we will remove that one from the feature list. There are still 22 features in the dataset, but we will only be using 21 of them for the rest of the analysis. This brings us back to 21 features, essentially replacing 'email_address' with 'fraction_poi_emails'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding 'fraction_poi_emails' to features_list, and removing 'email_address'\n",
    "features_list = [\n",
    " 'poi',\n",
    " 'bonus',\n",
    " 'deferral_payments',\n",
    " 'deferred_income',\n",
    " 'director_fees',\n",
    " 'exercised_stock_options',\n",
    " 'expenses',\n",
    " 'from_messages',\n",
    " 'from_poi_to_this_person',\n",
    " 'from_this_person_to_poi',\n",
    " 'loan_advances',\n",
    " 'long_term_incentive',\n",
    " 'other',\n",
    " 'restricted_stock',\n",
    " 'restricted_stock_deferred',\n",
    " 'salary',\n",
    " 'shared_receipt_with_poi',\n",
    " 'to_messages',\n",
    " 'total_payments',\n",
    " 'total_stock_value',\n",
    " 'fraction_poi_emails'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Best Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Now with out whole list of cleaned and updated features, we can select the most important ones to run though the classifiers. We will run SelectKBest to find the most important features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select K Best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating data\n",
    "data = featureFormat(my_dataset, features_list)\n",
    "labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['poi',\n",
       " 'deferral_payments',\n",
       " 'director_fees',\n",
       " 'exercised_stock_options',\n",
       " 'from_this_person_to_poi',\n",
       " 'loan_advances',\n",
       " 'other',\n",
       " 'restricted_stock_deferred',\n",
       " 'salary',\n",
       " 'to_messages',\n",
       " 'total_payments',\n",
       " 'total_stock_value']"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "selector = SelectKBest(k = 12)\n",
    "selectedFeatures = selector.fit(features,labels)\n",
    "feature_names = [features_list[i] for i in selectedFeatures.get_support(indices=True)]\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating our features list with these most important features.\n",
    "features_list = [\n",
    " 'poi',\n",
    " 'deferral_payments',\n",
    " 'director_fees',\n",
    " 'exercised_stock_options',\n",
    " 'from_this_person_to_poi',\n",
    " 'loan_advances',\n",
    " 'other',\n",
    " 'restricted_stock_deferred',\n",
    " 'salary',\n",
    " 'to_messages',\n",
    " 'total_payments',\n",
    " 'total_stock_value'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updating data\n",
    "data = featureFormat(my_dataset, features_list)\n",
    "labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Best Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into training and testing data\n",
    "from sklearn.model_selection import train_test_split\n",
    "features_train, features_test, labels_train, labels_test = \\\n",
    "    train_test_split(features, labels, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Gaussian Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "\n",
    "# Training Gaussian Naive Bayes\n",
    "clf = clf.fit(features_train, labels_train)\n",
    "pred = clf.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.3953488372093023\n",
      "Precision Score: 0.1724137931034483\n",
      "Recall Score: 0.7142857142857143\n"
     ]
    }
   ],
   "source": [
    "# Testing Gaussian Naive Bayes\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc = accuracy_score(labels_test, pred)\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "prec = precision_score(labels_test, pred)\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "rec = recall_score(labels_test, pred)\n",
    "\n",
    "print ('Accuracy Score: ' + str(acc))\n",
    "print ('Precision Score: ' + str(prec))\n",
    "print ('Recall Score: ' + str(rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# Training Decision Tree\n",
    "clf = clf.fit(features_train, labels_train)\n",
    "pred = clf.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.7209302325581395\n",
      "Precision Score: 0.2727272727272727\n",
      "Recall Score: 0.42857142857142855\n"
     ]
    }
   ],
   "source": [
    "# Testing Decision Tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc = accuracy_score(labels_test, pred)\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "prec = precision_score(labels_test, pred)\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "rec = recall_score(labels_test, pred)\n",
    "\n",
    "print ('Accuracy Score: ' + str(acc))\n",
    "print ('Precision Score: ' + str(prec))\n",
    "print ('Recall Score: ' + str(rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing SVM\n",
    "from sklearn import svm\n",
    "clf = svm.LinearSVC()\n",
    "\n",
    "# Training SVM\n",
    "clf = clf.fit(features_train, labels_train)\n",
    "pred = clf.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.7674418604651163\n",
      "Precision Score: 0.0\n",
      "Recall Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Testing SVM\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc = accuracy_score(labels_test, pred)\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "prec = precision_score(labels_test, pred)\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "rec = recall_score(labels_test, pred)\n",
    "\n",
    "print ('Accuracy Score: ' + str(acc))\n",
    "print ('Precision Score: ' + str(prec))\n",
    "print ('Recall Score: ' + str(rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression()\n",
    "\n",
    "# Training Logistic Regression\n",
    "clf = clf.fit(features_train, labels_train)\n",
    "pred = clf.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.8372093023255814\n",
      "Precision Score: 0.5\n",
      "Recall Score: 0.14285714285714285\n"
     ]
    }
   ],
   "source": [
    "# Testing Logistic Regression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc = accuracy_score(labels_test, pred)\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "prec = precision_score(labels_test, pred)\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "rec = recall_score(labels_test, pred)\n",
    "\n",
    "print ('Accuracy Score: ' + str(acc))\n",
    "print ('Precision Score: ' + str(prec))\n",
    "print ('Recall Score: ' + str(rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Nearest K\n",
    "from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "clf = NearestCentroid()\n",
    "\n",
    "# Training Nearest K\n",
    "clf = clf.fit(features_train, labels_train)\n",
    "pred = clf.predict(features_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.8372093023255814\n",
      "Precision Score: 0.5\n",
      "Recall Score: 0.42857142857142855\n"
     ]
    }
   ],
   "source": [
    "# Testing Nearest K\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc = accuracy_score(labels_test, pred)\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "prec = precision_score(labels_test, pred)\n",
    "\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "rec = recall_score(labels_test, pred)\n",
    "\n",
    "print ('Accuracy Score: ' + str(acc))\n",
    "print ('Precision Score: ' + str(prec))\n",
    "print ('Recall Score: ' + str(rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Out of all the classifiers tested, the NearestK is the most accurate, therefore that is the classifier we will use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Dumping classifier, dataset, and features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_classifier_and_data(clf, my_dataset, features_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The goal of this project was to create a PoI identifier based on public Enron financial and email data. This can be created by finding the attributes that PoI have in common, and attributes that non PoIs have in common. Loading these into a machine learning algorithm, and the algorithm can try to detect who might be a PoI and who might not. There were several outliers that had to be removed, such as data in the wrong place, and a total summary of all data that was not needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The feature I created was the percentage of PoI emails to other PoIs, compared the the total emails from anyone to anyone. This did not change the algorithm predictions very much. I then used Select K Best to find the most important features from the data. I selected the best 12 to use. This seemed like a good compromise from the 22 total features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The algorithm with the highest overall scores was the Nearest K algorithm. It had better performance in general when compared to others I tried such as Decision Trees and SVM. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well? How did you tune the parameters of your particular algorithm? What parameters did you tune? (Some algorithms do not have parameters that you need to tune -- if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> An algorithms parameters must be tuned to a certain extent. Too much or too little can lead to innacurate data and overfitting. The nearest centroid algorithm that I used did not need any addition tuning besides the selecting of the most important features. When using a decision tree it would be necessary to tune the parameters and try many different tests to find the most optimal parameters. Tuning parameters to manage things such as entropy, weight, impurity, etc. Through my testing, none of the parameter changes made the decision tree any more accurate than the Nearest Centroid, so I used that algorithm, and left the Decision Tree as the default just for the example comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Validation is the processed of checking to see how your model performs on unseen data. A classic mistake would be tuning your model be able to predict your training data very well , but then having it perform poorly on unseen out-of-sample testing data. This is called overfitting. One of the major goals in validation is to avoid overfitting, which can be accomplished through a process called cross-validation. This analysis was validated though Sklearns Train/Test/Split algorithm. This splits the data into both training and testing data to be used to train and test the algorithms and compare their performance to the actual data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Give at least 2 evaluation metrics and your average performance for each of them. Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The Nearest Centroid with the specific features was able to get an Accuracy score of ~ 84%, a Precision score of 50%, and a Recall score of ~ 43%. These scores, specifially the accuracy score, gives about a 84% change for the algorithm to essentially guess who might be a PoI and who might not. This is significantly better than just guessing who might be a PoI by random selection of any of the 146 original people in the dataset, which would be more like less than 1% chance of a correct guess, based on no further information. However, by training the algorithm, it can achieve a fairly high accuracy in choosing who is a PoI and who is not."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
